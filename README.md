# mola-expert
Implementation of MoLA (Mixture of LoRA Experts) for fine-tuning large language models. Supports expert routing, variable experts per layer, and integration with Hugging Face SFTTrainer. Includes example with Qwen2.5 and financial translation dataset for scalable, efficient adaptation.
